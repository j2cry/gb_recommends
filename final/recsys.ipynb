{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "from featuring import Merger, ColumnsCorrector, LastDaysRate, BasketRate, DepartmentSellRate, SameDepartmentPurchases, \\\n",
    "    MeanDepartmentExpenses\n",
    "\n",
    "from datasplit import DataSplit, prepare_true_values\n",
    "from preprocess import DataPreprocessor\n",
    "from candidate_model import CandidateModel\n",
    "from metrics import precision_at_k, recall_at_k, calc_mean_metric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## prepare data & fit candidate model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "purchases = pd.read_csv('retail_train.csv')\n",
    "\n",
    "# train/valid split\n",
    "splitter = DataSplit(purchases, 'week_no', [6, 4])\n",
    "\n",
    "train_lv1 = purchases[splitter.part0].copy()\n",
    "valid_lv1 = purchases[splitter.part1].copy()\n",
    "valid_lv2 = purchases[splitter.part2].copy()\n",
    "\n",
    "# prepare lv1 validation true values\n",
    "true_train_lv1 = prepare_true_values(train_lv1)\n",
    "true_valid_lv1 = prepare_true_values(valid_lv1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "mix_feat_params = {\n",
    "    'top_config': {'fields': ['quantity', 'sales_value'],\n",
    "                   'beta': [1., 1.],\n",
    "                   'k': 5000,\n",
    "                   'scaler': StandardScaler\n",
    "                    },\n",
    "    'uim_config': {'aggfunc': 'sum',\n",
    "                   # 'weights': tfidf_weight\n",
    "                   },\n",
    "}\n",
    "\n",
    "pre = DataPreprocessor(train_lv1, **mix_feat_params)\n",
    "pre.fit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.30147727453621564, 0.2123693837009425)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_params = {\n",
    "    'train': pre.train_uim_sparse,\n",
    "    'weighted': pre.train_uim_weighted,\n",
    "    'top_items': pre.top_k_items,\n",
    "    'placeholder_id': pre.placeholder_id,\n",
    "    'idx_to_item': pre.idx_to_item,\n",
    "    'item_to_idx': pre.item_to_idx,\n",
    "    'user_to_idx': pre.user_to_idx\n",
    "}\n",
    "n_candidates = 100\n",
    "cm = CandidateModel('BM25', **candidate_params)\n",
    "cm.fit(K=1)\n",
    "\n",
    "train_candidates = cm.predict(true_train_lv1['user_id'], N=n_candidates)\n",
    "valid_candidates = cm.predict(true_valid_lv1['user_id'], N=n_candidates)\n",
    "\n",
    "recall_lv1_train = calc_mean_metric(recall_at_k, true_train_lv1['actual'], train_candidates, k=n_candidates)\n",
    "recall_lv1_valid = calc_mean_metric(recall_at_k, true_valid_lv1['actual'], valid_candidates, k=n_candidates)\n",
    "recall_lv1_train, recall_lv1_valid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(249900, 2)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect candidates for ALL existing users\n",
    "all_users = pd.Series(purchases['user_id'].sort_values().unique(), name='user_id')    # sort здесь не обязателен, но удобен при отладке\n",
    "pred_candidates = cm.predict(all_users, N=n_candidates)\n",
    "candidates = pd.DataFrame.from_dict(pred_candidates.to_dict(), orient='index').set_index(all_users)\n",
    "candidates = candidates.stack().reset_index(level=1, drop=True).rename('item_id').reset_index()\n",
    "candidates.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "0.15684617242212026"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# общая доля релевантных товаров среди кандидатов в обучающей выборке\n",
    "relevant = valid_lv1[['user_id', 'item_id']].copy()\n",
    "relevant['target'] = 1\n",
    "zeros, ones = candidates.merge(relevant, on=['user_id', 'item_id'], how='left').fillna(0)['target'].value_counts()\n",
    "ones / (zeros + ones)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## prepare data for lv2 model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def merge_candidates(df, cand, users=None):\n",
    "    \"\"\" Prepare dataset lv2 for featuring\n",
    "    :param df: required data to be prepared\n",
    "    :param cand: dataset with stacked candidates\n",
    "    :param users: leave only specified users\n",
    "    \"\"\"\n",
    "    if users is not None:\n",
    "        warm = df['user_id'].isin(users)\n",
    "        target = df[warm].copy()\n",
    "    else:\n",
    "        target = df.copy()\n",
    "    required_users = cand['user_id'].isin(target['user_id'].unique())       # keep candidates for only required users\n",
    "    target['target'] = 1      # flag means this item was really bought\n",
    "    target = cand[required_users].merge(target, on=['user_id', 'item_id'], how='left').fillna(0)\n",
    "    return target.drop(columns='target'), target['target']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "((230524, 12), (211663, 12))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train lv2: prepare for featuring (markup & merge candidates)\n",
    "train_lv2_merged, train_lv2_target = merge_candidates(valid_lv1, candidates)    # both warm & cold: candidates for cold users are predicted from top5k\n",
    "\n",
    "# valid lv2: prepare for featuring (markup & merge candidates)\n",
    "valid_lv2_merged, valid_lv2_target = merge_candidates(valid_lv2, candidates)\n",
    "train_lv2_merged.shape, valid_lv2_merged.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### feature engineering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# # baseline item features\n",
    "# drop_columns = ['basket_id', 'day', 'quantity', 'sales_value', 'store_id', 'retail_disc', 'trans_time', 'week_no', 'coupon_disc', 'coupon_match_disc']\n",
    "# item_data = pd.read_csv('product.csv')\n",
    "# item_data.columns = item_data.columns.str.lower()\n",
    "# item_data.rename(columns={'product_id': 'item_id'}, inplace=True)\n",
    "# # item_data.drop(columns=['curr_size_of_product'], inplace=True)\n",
    "# keep_cols = ['item_id', 'department',]\n",
    "# item_data = item_data[keep_cols]\n",
    "#\n",
    "# # baseline user features\n",
    "# user_data = pd.read_csv('../hw5/user_features_corrected.csv')\n",
    "#\n",
    "# # merge dummies\n",
    "# featured_train_lv2 = train_lv2.copy()\n",
    "# featured_train_lv2 = featured_train_lv2.merge(pd.get_dummies(item_data), on='item_id', how='left').fillna(0)\n",
    "# featured_train_lv2 = featured_train_lv2.merge(user_data, on='user_id', how='left').fillna(0)\n",
    "# featured_train_lv2.drop(columns=drop_columns, inplace=True)\n",
    "#\n",
    "# featured_valid_lv2 = valid_lv2.copy()\n",
    "# featured_valid_lv2 = featured_valid_lv2.merge(pd.get_dummies(item_data), on='item_id', how='left').fillna(0)\n",
    "# featured_valid_lv2 = featured_valid_lv2.merge(user_data, on='user_id', how='left').fillna(0)\n",
    "# featured_valid_lv2.drop(columns=drop_columns, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# load items data\n",
    "item_data = pd.read_csv('product.csv')\n",
    "item_data.columns = item_data.columns.str.lower()\n",
    "item_data.rename(columns={'product_id': 'item_id'}, inplace=True)\n",
    "\n",
    "# load users data\n",
    "user_data = pd.read_csv('hh_demographic.csv')\n",
    "user_data.columns = user_data.columns.str.lower()\n",
    "user_data.rename(columns={'household_key': 'user_id'}, inplace=True)\n",
    "\n",
    "# load prepared user features\n",
    "user_data_corrected = pd.read_csv('../hw5/user_features_corrected.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "drop_columns = ['basket_id', 'day', 'quantity', 'sales_value', 'store_id', 'retail_disc', 'trans_time', 'week_no',\n",
    "                'coupon_disc', 'coupon_match_disc']\n",
    "keep_user_cols = ['hh_comp', 'hh_size', 'kids', 'single_female', 'single_male']\n",
    "\n",
    "featuring = Pipeline([('UserFeaturesMerger', Merger(user_data_corrected, on='user_id', cols=keep_user_cols)),\n",
    "                      # ('LastDaysRate', LastDaysRate(n_days=25)),\n",
    "                      # ('BasketRate', BasketRate(n_days=14)),\n",
    "                      ('MeanDepartmentExpenses', MeanDepartmentExpenses(item_data, n_days=28)),\n",
    "                      ('DepartmentSellRate', DepartmentSellRate(item_data, n_days=7)),\n",
    "                      ('SameDepartmentPurchases', SameDepartmentPurchases(item_data, n_days=3)),\n",
    "                      # ('', ),\n",
    "                      # ('', ),\n",
    "                      ('drop', ColumnsCorrector(drop_columns, mode='drop')),\n",
    "                      ])\n",
    "\n",
    "featuring.fit(train_lv2_merged)\n",
    "featured_train_lv2 = featuring.transform(train_lv2_merged)\n",
    "featured_valid_lv2 = featuring.transform(valid_lv2_merged)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## fit lv2 LGBM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.9664045746962115,\n 0.9606986899563319,\n 0.032525019245573515,\n 0.03205711995920003)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LGBMClassifier(#max_depth=3,\n",
    "                       num_leaves=13,\n",
    "                       learning_rate=0.00625,\n",
    "                       n_estimators=150,\n",
    "                       random_state=193, n_jobs=-1)\n",
    "model.fit(featured_train_lv2, train_lv2_target)\n",
    "\n",
    "train_pred = model.predict(featured_train_lv2)\n",
    "valid_pred = model.predict(featured_valid_lv2)\n",
    "lgb_pr_train = precision_score(train_lv2_target, train_pred)\n",
    "lgb_pr_valid = precision_score(valid_lv2_target, valid_pred)\n",
    "lgb_rc_train = recall_score(train_lv2_target, train_pred)\n",
    "lgb_rc_valid = recall_score(valid_lv2_target, valid_pred)\n",
    "\n",
    "lgb_pr_train, lgb_pr_valid, lgb_rc_train, lgb_rc_valid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def predict_recommends(data, *, k):\n",
    "    \"\"\" Rank and collect candidates for given users \"\"\"\n",
    "    keep_users = candidates['user_id'].isin(data['user_id'].unique())\n",
    "    data_merged, _ = merge_candidates(data, candidates[keep_users])\n",
    "    data_featured = featuring.transform(data_merged)\n",
    "    true_values = prepare_true_values(data_merged)\n",
    "\n",
    "    proba = pd.Series(model.predict_proba(data_featured).T[1], name='proba')\n",
    "    ranked_pred = pd.concat([data_featured[['user_id', 'item_id']], proba], axis=1)\n",
    "    # collect recommends\n",
    "    sorted_cand = candidates[keep_users].merge(ranked_pred, on=['user_id', 'item_id'], how='left')\\\n",
    "        .sort_values(by=['user_id', 'proba'], ascending=[True, False])\\\n",
    "        .groupby('user_id').head(k)\n",
    "    predicts = sorted_cand.groupby('user_id')['item_id'].unique()\n",
    "\n",
    "    # calc rank metric\n",
    "    metric = calc_mean_metric(precision_at_k, true_values['actual'], predicts.reset_index(drop=True), k=k)\n",
    "    # fill missing predictions from top K items\n",
    "    predicts = cm.fill_from_top(predicts, k)\n",
    "    # можно считать rank_metric тут - но тогда она будет включать неранжированные товары (и возможно, будет выше)\n",
    "    # metric = calc_mean_metric(precision_at_k, true_values['actual'], predicts.reset_index(drop=True), k=k)\n",
    "    return predicts, metric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# рассчитываем два варианта метрик\n",
    "k = 5\n",
    "true_train_lv2_real = prepare_true_values(valid_lv1)\n",
    "true_valid_lv2_real = prepare_true_values(valid_lv2)\n",
    "\n",
    "train_rec, train_rank_precision = predict_recommends(valid_lv1, k=k)\n",
    "valid_rec, valid_rank_precision = predict_recommends(valid_lv2, k=k)\n",
    "precision_train_lv2 = calc_mean_metric(precision_at_k, true_train_lv2_real['actual'], train_rec.reset_index(drop=True), k=k)\n",
    "precision_valid_lv2 = calc_mean_metric(precision_at_k, true_valid_lv2_real['actual'], valid_rec.reset_index(drop=True), k=k)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.8355617455896008, 0.8992164544564152)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Эта метрика показывает, насколько точно (pr@5) предикты соответствуют отобранным кандидатам\n",
    "train_rank_precision, valid_rank_precision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.2700092850510678, 0.25592556317335946)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# А эта - насколько (pr@5) рекомендации совпадают с реальными данными покупок\n",
    "precision_train_lv2, precision_valid_lv2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "В предыдущих работах максимальная метрика была ~0.22262 для ALS с BM25 взвешиванием (см. hw3).\n",
    "Так что текущая 0.256 это вполне ощутимый прирост (почти на 15%)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# save models\n",
    "cloudpickle.dump(cm, open('models/candidate_model.pkl', 'wb'))\n",
    "cloudpickle.dump(featuring, open('models/featuring_pipeline.pkl', 'wb'))\n",
    "cloudpickle.dump(model, open('models/rank_model.pkl', 'wb'))\n",
    "\n",
    "# где-то в процессе обработки предиктов list видимо заменяется на массив другого типа, из-за чего криво преобразуется в str\n",
    "# поэтому сделаем принудительный рекаст\n",
    "valid_rec = valid_rec.apply(list)\n",
    "valid_rec.to_csv('recommendations.csv')     # recommendations for 2042 users from lv2 validation data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "item_id    0.255926\ndtype: float64"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# читаем предикты из файла\n",
    "valid_rec_check = pd.read_csv('recommendations.csv', index_col='user_id', converters={'item_id': pd.eval})\n",
    "# проверяем, что метрика не изменилась - значит все сохранилось/считалось правильно\n",
    "calc_mean_metric(precision_at_k, true_valid_lv2_real['actual'], valid_rec_check.reset_index(drop=True), k=k)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}