{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Q: В чем принципиальное отличие гибридных рекомендательных систем от коллаборативной филтьтрации? Приведите 2-3 примера задач, в которых необходимо использовать гибридные системы.\n",
    "\n",
    "A: Поскольку коллаборативная фильтрация основана на матричной факторизации, она учитывает только реальные user-item взаимодействия (купил/не купил), игнорируя остальные доступные параметры (бренды товаров, время совершения покупки, частота покупок, возраст пользователей, их социальный статус, etc). В связи с этим возникает проблема \"холодного старта\": выдачи рекомендаций новым или мало активным пользователям, а также рекомендация новых товаров. И в предыдущих работах этого курса можно заметить, что некоторые алгоритмы выдают меньше предсказаний, чем необходимо. Гибридная рекомендательная система решает эти проблемы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Q: Прочитайте статью про поиск на hh.ru https://habr.com/ru/company/hh/blog/347276/ Нам интересна именно рекомендательная система, раздел “Производительность системы” можно пропустить Какие основные отличия предложенной системы от тех подходов, которые мы разбирали на семинарах? Какие проблемы могут возникнуть при выводе такой модели в продакшен?\n",
    "\n",
    "A: Описанная рекомендательная система HH состоит (в общем виде) из 4-х элементов (см. рис. Схема работы рекомендательной системы). На вебинарах мы рассматривали состоящие только из 2-х (ALS similar-user + item-item). К сожалению, в статье нет подробного описания работы каждого этапа моделирования, но в качестве примера сложностей в продакшене предположу следующее: эвристический фильтр на 2-х признаках и фльтрующая модель 1 - на 4-х - работают, скорее всего быстро, но насколько качественно - вопрос. Также не очень понятно, как решается проблема индексации и генерации признаков для неполных или некорректно заполненных вакансий/резюме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Q: На вебинаре мы рассматривали модель LightFM (https://making.lyst.com/lightfm/docs/lightfm.html). В работе Data Scientist’а важную часть занимает research - исследование существующих архитектур и разбор научных статей, в которых они описываются. Вам предлагается изчуть оригинальную статью про LightFM https://arxiv.org/pdf/1507.08439.pdf и ответить на следующие вопросы:\n",
    "1) Какой датасет используют авторы?\n",
    "2) Что используют в качестве признаков?\n",
    "3) С какими моделями сравнивают LightFM? Опишите их основные идеи кратко\n",
    "\n",
    "A: \n",
    "1) Автор использует два датасета:\n",
    "\tMovieLens: ~10млн оценок фильмов выставленных 71,5к пользователями по 10,6к фильмам\n",
    "\tCrossValidated: Q&A датасет из 44,2к вопросов, 1032 уникальных тегов, 188,8к ответов и комментариев, оставленных 5,9к пользователями.\n",
    "\n",
    "2) Эксперимент с LightFM в статье разделен на три сегмента по набору обрабатываемых фичей:\n",
    "\tВ первом в качестве фичей используются только теги фильмов для 1го датасетя, вопросов - для 2го.\n",
    "\tВо втором - теги и данные по взаимодействияс с товарами.\n",
    "\tВ третьем используются теги и информация о пользователях.\n",
    "    В первых двух вариантах информация о пользователях представлена только матрицей взаимодействий.\n",
    "\n",
    "3) Автор сравнивает LightFM с обычной матричной факторизацией (MF) и двумя усложненными моделями: LSI-LR, LSI-UP\n",
    "LSI-LR: модель на основе контента, с помощью латентно-семантического анализа сегментирует items, выделяя тематики; объекты представляет как линейные комбинации тематик. Затем для каждого пользователя строится логистическая регрессия, определяющаяя для него пространство тематик.\n",
    "\n",
    "LSI-UP: гибридная модель, представляющая профили пользователей в виде линейных комбинаций векторов товаров. Для получения представлений пользователей и товаров использует матричную факторизацию. Насколько понимаю, это сингулярное разложение с добавленной user-сегментацией.\n",
    "\n",
    "LightFM включает в себя преимущества как MF, так и CB моделей. В частности когда обучающие данные не содержат фичей товаров/пользователей, LightFM работает как обычная матричная факторизация. Если же имеются пересекающиеся метаданные (т.е. относящиеся к более чем одному пользователю/товару), LightFM на их основе пересчитывает эмбеддинги, что позволяет обеспечить предсказания на холодном старте.\n",
    "Также, в большинстве случаев, количество метаданных значительно ниже, чем кол-во пользователей/товаров, что снижает риск переобучения.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k, recall_at_k\n",
    "\n",
    "from additional import DataProcessor\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from hyperopt import hp, fmin, tpe\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## load & split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load purchases\n",
    "purchases = pd.read_csv('retail_train.csv')\n",
    "\n",
    "# train/test split\n",
    "test_size_weeks = 3\n",
    "train = purchases[purchases['week_no'] < purchases['week_no'].max() - test_size_weeks].copy()\n",
    "test = purchases[purchases['week_no'] >= purchases['week_no'].max() - test_size_weeks].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA в отдельном блокноте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "подготовим параметры обработки датасета:\n",
    "* defaults: на основе кол-ва проданных товаров\n",
    "* mix_feat: на комбинации стоимости и кол-ва проданных товаров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mix_feat_params = {\n",
    "    'top_config': {'fields': ['quantity', 'sales_value'],\n",
    "                   'beta': [1., 1.],\n",
    "                   'k': 5000,\n",
    "                   'scaler': StandardScaler},\n",
    "    'uim_config': {'aggfunc': 'sum',\n",
    "                   'weights': bm25_weight\n",
    "                   },\n",
    "}\n",
    "\n",
    "defaults_params = {\n",
    "    'top_config': {'fields': ['quantity'],\n",
    "                   'k': 5000},\n",
    "    'uim_config': {'aggfunc': 'count',\n",
    "    #             #    'weights': bm25_weight\n",
    "                   },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# создаем хранилище обучающих и валидационных данных\n",
    "preparer = DataProcessor(train, test, **mix_feat_params)\n",
    "preparer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Item featuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# baseline\n",
    "# load items data\n",
    "item_data = pd.read_csv('product.csv')\n",
    "item_data.columns = item_data.columns.str.lower()\n",
    "item_data.rename(columns={'product_id': 'item_id'}, inplace=True)\n",
    "\n",
    "# # in_top100 feature\n",
    "# in_top100 = item_data['item_id'].isin(preparer.top_k_items[:100])\n",
    "# item_data['in_top100'] = 0\n",
    "# item_data.loc[in_top100, 'in_top100'] = 1\n",
    "# # drop columns\n",
    "# item_data.drop(columns=['curr_size_of_product'], inplace=True)\n",
    "\n",
    "# keep_cols = ['item_id', 'manufacturer', 'department', 'brand']\n",
    "# item_data = item_data[keep_cols]\n",
    "\n",
    "# dummy\n",
    "item_features = pd.DataFrame(preparer.train_uim.columns)\n",
    "item_features = item_features.merge(item_data, on='item_id', how='left')\n",
    "item_features.set_index('item_id', inplace=True)\n",
    "item_features = pd.get_dummies(item_features, columns=item_features.columns.tolist())\n",
    "del item_data\n",
    "# item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## User featuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # baseline user/item features\n",
    "# # load users data\n",
    "# user_data = pd.read_csv('hh_demographic.csv')\n",
    "# user_data.columns = user_data.columns.str.lower()\n",
    "# user_data.rename(columns={'household_key': 'user_id'}, inplace=True)\n",
    "\n",
    "# # dummy\n",
    "# user_features = pd.DataFrame(preparer.train_uim.index)\n",
    "# user_features = user_features.merge(user_data, on='user_id', how='left')\n",
    "# user_features.set_index('user_id', inplace=True)\n",
    "# user_features = pd.get_dummies(user_features, columns=user_features.columns.tolist())\n",
    "# # del user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Загружаем user features, их подготовка - в одноименном блокноте\n",
    "user_data = pd.read_csv('user_features_corrected.csv')\n",
    "user_features = pd.DataFrame(preparer.train_uim.index)\n",
    "user_features = user_features.merge(user_data, on='user_id', how='left').fillna(0)\n",
    "user_features.set_index('user_id', inplace=True)\n",
    "# del user_data\n",
    "# user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "# homeowner = user_data.columns[user_data.columns.str.match('homeowner')].to_list()\n",
    "# user_features.drop(columns=[*homeowner], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pr@5: 0.10024077445268631\n",
      "Test pr@5: 0.011842106468975544\n"
     ]
    }
   ],
   "source": [
    "# model_params = {\n",
    "#     'no_components': 10,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'item_alpha': 0.1,\n",
    "#     'user_alpha': 0.1\n",
    "# }\n",
    "model_params = {\n",
    "    \"no_components\": 24,\n",
    "    \"learning_rate\": 0.33134377654955266,\n",
    "    \"item_alpha\": 0.04194646712501058,\n",
    "    \"user_alpha\": 0.3439340673250181}\n",
    "\n",
    "model = LightFM(loss='warp', # 'bpr'\n",
    "                random_state=42,\n",
    "                **model_params)\n",
    "\n",
    "model.fit((preparer.train_uim_sparse > 0) * 1,  # user-item matrix из 0 и 1\n",
    "          sample_weight=coo_matrix(preparer.train_uim_weighted),\n",
    "          user_features=csr_matrix(user_features.values).tocsr(),\n",
    "          item_features=csr_matrix(item_features.values).tocsr(),\n",
    "          epochs=15)\n",
    "\n",
    "train_pr = precision_at_k(model, preparer.train_uim_sparse, k=5,\n",
    "                          user_features=csr_matrix(user_features.values),\n",
    "                          item_features=csr_matrix(item_features.values)).mean()\n",
    "\n",
    "test_pr = precision_at_k(model, preparer.test_uim_sparse, k=5,\n",
    "                         user_features=csr_matrix(user_features.values).tocsr(),\n",
    "                         item_features=csr_matrix(item_features.values).tocsr()).mean()\n",
    "\n",
    "print(f'Train pr@5: {train_pr}', f'Test pr@5: {test_pr}', sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# item_index = np.arange(preparer.train_uim.columns.size)\n",
    "# predictions = model.predict(user_ids=6, item_ids=item_index,\n",
    "#                             user_features=csr_matrix(user_features.values).tocsr(),\n",
    "#                             item_features=csr_matrix(item_features.values).tocsr(),\n",
    "#                             num_threads=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "hopt_history = []\n",
    "hopt_metrics = []\n",
    "\n",
    "# define objective function\n",
    "def objective(params):\n",
    "    model = LightFM(**params, loss='warp', random_state=42)\n",
    "\n",
    "    model.fit((preparer.train_uim_sparse > 0) * 1,  # user-item matrix из 0 и 1\n",
    "            sample_weight=coo_matrix(preparer.train_uim_weighted),\n",
    "            user_features=csr_matrix(user_features.values).tocsr(),\n",
    "            item_features=csr_matrix(item_features.values).tocsr(),\n",
    "            epochs=15)\n",
    "\n",
    "    _pr = precision_at_k(model, preparer.test_uim_sparse, k=5,\n",
    "                         user_features=csr_matrix(user_features.values).tocsr(),\n",
    "                         item_features=csr_matrix(item_features.values).tocsr()).mean()\n",
    "    hopt_history.append(params)\n",
    "    hopt_metrics.append(_pr)    \n",
    "    return 1 / _pr if _pr else np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a search space\n",
    "search_space = {'no_components': 5 + hp.randint('no_components', 45),\n",
    "                'learning_rate': hp.uniform('learning_rate', 1e-4, 0.4),\n",
    "                'item_alpha': hp.uniform('item_alpha', 0, 0.4),\n",
    "                'user_alpha': hp.uniform('user_alpha', 0, 0.4),\n",
    "                }\n",
    "\n",
    "static_params = {'loss': 'warp',\n",
    "                 'random_state': 42,\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# searching\n",
    "best = fmin(objective, search_space, algo=tpe.suggest, max_evals=5)\n",
    "best.update(static_params)\n",
    "hopt_history[np.array(hopt_metrics).argmax()], max(hopt_metrics)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [30:27<00:00, 182.73s/trial, best loss: 120.4877971161931]\n",
      "100%|██████████| 10/10 [22:20<00:00, 134.07s/trial, best loss: 145.29411595959257]\n",
      "100%|██████████| 10/10 [26:11<00:00, 157.14s/trial, best loss: 84.44443584592342]\n",
      "100%|██████████| 10/10 [26:16<00:00, 157.61s/trial, best loss: 133.51351098630275]\n",
      "100%|██████████| 10/10 [24:23<00:00, 146.39s/trial, best loss: 96.8627352350298] \n",
      "CPU times: user 2h 4min 5s, sys: 26.4 s, total: 2h 4min 32s\n",
      "Wall time: 2h 9min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'metric': 0.011842106468975544,\n",
       " 'params': {'item_alpha': 0.04194646712501058,\n",
       "  'learning_rate': 0.33134377654955266,\n",
       "  'no_components': 24.0,\n",
       "  'user_alpha': 0.3439340673250181}}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# поиск в несколько подходов\n",
    "n_cycles = 5\n",
    "best_arr = []       # набор параметров, которые алгоритм счел лучшими\n",
    "\n",
    "saved_best = json.load(open('hypopt.json'))\n",
    "for _ in range(n_cycles):\n",
    "    best = fmin(objective, search_space, algo=tpe.suggest, max_evals=10)\n",
    "    best.update(static_params)\n",
    "    best_arr.append(best)\n",
    "\n",
    "    params, metric = hopt_history[np.array(hopt_metrics).argmax()], max(hopt_metrics)\n",
    "    if metric > saved_best['metric']:\n",
    "        saved_best = {'metric': float(metric), 'params': {k: float(v) for k, v in params.items()}}\n",
    "    json.dump(saved_best, open('hypopt.json', 'w'))\n",
    "\n",
    "saved_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline: 0.4366 / 0.0026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"metric\": 0.011842106468975544, \"params\": {\"item_alpha\": 0.04194646712501058, \"learning_rate\": 0.33134377654955266, \"no_components\": 24.0, \"user_alpha\": 0.3439340673250181}}\n",
    "\n",
    "({'item_alpha': 0.39985035877099606,\n",
    "  'learning_rate': 0.29425384326656523,\n",
    "  'no_components': 9,\n",
    "  'user_alpha': 0.3729632462959629},\n",
    " 0.0114372475)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
